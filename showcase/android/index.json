[{"content":"Written in Go, Hugo is an open source static site generator available under the Apache Licence 2.0. Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.\nHugo makes use of a variety of open source projects including:\n https://github.com/russross/blackfriday https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper  Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.\nHugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.\nWebsites built with Hugo are extremelly fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.\nLearn more and contribute on GitHub.\n","description":"Hugo, the world’s fastest framework for building websites","id":1,"section":"","tags":null,"title":"About","uri":"https://zisistach.org/about/"},{"content":"Introduction Most of the recommendations presented below are coming directly from the web (StackOverflow, Solr wiki, Solr Jira, Solr mailing list), while some others required a lot of testing and debugging in order to understand them and reach a solution. It’s also nowhere complete but I hope it can give some directions to people facing performance issues with Solr. In this post we’ll focus on SolrCloud stability and search performance. There are even more Solr performance related storied to tell.\nCluster instability The following points fall under the “abusing Solr” category. The same way that JOIN functionality provided by RDBMSs does not mean that all the JOIN queries you can imagine will be performant, there are some usages of Solr that could cause headaches.\nDo not ask for too many results in a single call Avoid setting Integer.MAX or high values (usually when doing paging with start and rows params) in .setRows() call when preparing a SolrQuery. It can wreak havoc to the Solr cluster. Basically Solr tries to allocate as much memory as indicated in the param passed to .setRows(), not a good thing. Haven’t seen it with my own eyes happening but others have. Check Solr Wiki https://cwiki.apache.org/confluence/display/SOLR/SolrPerformanceProblems.\nAlternatives:\n For real-time requests first get the # of docs in the index and pass this to .setRows() instead. For calls that you know what to expect just pass the expected number of results. For “next-page” functionality and offline procedures cursorMark can be utilized.  Avoid deleteByQuery calls as much as possible Calling intermixed calls to indexing and deleteByQuery() is just asking for trouble. If you witness seemingly random replicas crashes and/or getting in recovery mode it might be related to deleteByQuery calls interspersed with indexing requests.\nAlternatives:\n Make use of deleteById() directly if possible. Try to search first using the query that is passed to deleteByQuery(), get the UniqueIDs and call deleteById(). Get rid of your deletes altogether, by making use of atomic updates. This comes with its own limitations but you can check if it fits your use-case.  Improving search throughput The need for sharding and maxConnectionsPerHost The story has as follows. Searching on a single-sharded SolrCloud setup with a 10M docs/350GB index and high cardinality fields was slow, especially when faceting was utilized. A simple search from Solr admin page with faceting enabled could take ~1.5sec.After the index was split into 8 shards the same query’s response time dropped to ~200msec! That’s great I thought, problem solved. But it wasn’t to be.\nRunning a load test showed that after a certain rate (\u0026gt;20QPS) performance dropped dramatically.So, what was going on? At first I noticed that having more than 2 shards on the same Solr JVM was causing slowness, but I could not figure out why this was happening. The CPUs on the machine were more than enough to server requests in multiple shards. Turns out there’s a property maxConnectionsPerHost inside solr.xml under the HttpShardHandlerFactory node. Increasing that (default value is 20) gives some fresh air to Solr and inter-shard communication and response times are back to expected values (~200msec) even under heavy load.\n***Note: This was in Solr 6.5.x . Nowadays this value is set to a high number by default ***\nCheck your FilterCache hit ratio (especially in distributed search) Searching in a distributed index makes heavy use of the FilterCache that is not evident when searching in a single shard. Faceting especially seems to generate and store multiple entries in FilterCache. So you really want to have a high hit ratio in your caches with FilterCache usually affecting performance the most. FilterCache size can be limited in 2 ways. Either by the max # of entries it can have or the max memory it should occupy through maxRAMMB. maxRAMMB has been linked to possible memory leaks (https://www.mail-archive.com/solr-user@lucene.apache.org/msg151463.html) so use with care. Usually I find it safer (although more cumbersome) to just tune the # of entries in the cache for your specific use case.\nProfile your custom tokenizers/filters You don’t really need any exotic tools for this one. Tools provided by the JDK (like JVisualVM) can monitor threads/memory activity while you index/search. Look for your custom classes and how much time is spent on them.\nIf you have GC issues, try the G1 collector. It can do miracles. Witnessed a setup where the default collector used by Solr could not cope with the load after some time. Just switching to G1 fixed things. BUT usually it\u0026rsquo;s your code that causes issues to garbage collection. Either through memory leaks or by generating too many objects that do not allow the GC to keep up. Java Mission Control (https://www.theserverside.com/definition/Java-Mission-Control) can prove really helpful when trying to understand the rate and the type of objects that are getting created.\nUtilize facet.threads In practice the use of facet.threads has improved search response times when faceting is enabled but still sharding the index will probably yield better response times for faceting and search in general. There’s no point in setting this variable a higher value than the # of fields you are faceting on. It should at most be equal to the # of faceting fields. Note that this param is not applicable to JSON faceting.\nUse DocValues for faceting This comes directly from Solr Documentation, although I don’t have any serious metrics I’ll take their word for it.\nPay attention to your deleted documents Maybe you have a perfectly healthy SolrCloud instance which performs great. Time goes on and after a few months you notice a dramatic degradation in search performance. Code is the same, hardware is the same. It’s time to have a look at the percentage of the deleted documents in your index. They might be responsible for this degradation and you have to somehow get rid of them. Segments handling in Lucene/Solr as well as handling of deleted documents is a black art (or black magic for that matter). More info can be found at https://sematext.com/blog/solr-optimize-is-not-bad-for-you-lucene-solr-revolution/.\nNote: The issue has been fixed in Solr 7.5.x where the deletesPctAllowed param can be utilized inside solrconfig.xml\n","description":"A guide on common performance issues encountered in SolrCloud","id":3,"section":"posts","tags":["solrcloud","lucene","java","performance"],"title":"SolrCloud performance tuning","uri":"https://zisistach.org/posts/03_solr_performance/"},{"content":"Introduction Here is a guide on configuring an SFML, OpenGL, C++ project under Netbeans in MacOS X. I couldn’t find any similar guide on the net and I encountered some caveats and intricacies while trying to set it up. The lack of a complete guide alone made it worth blogging about.\nFirst of all, I assume that NetBeans and SFML are already installed on your machine. If not, please go to the corresponding product’s official site and follow the guides there. They are pretty straighforward. Also make sure that Xcode has the command-line tools installed in order to ensure that the g++ compiler is properly installed and configured. Once everything is in place let’s start setting up the project.\nCreating the Netbeans project Open Netbeans and click on “File \u0026gt; New Project…”. From the available options select the “C/C++ Application” and click “Next”.\nIn the following screen provide a name for your project in the “Project Name:” textbox and click on “Finish”. The default values for remaining options should be just fine.\nConfiguring the needed libraries We need to make visible SFML and OpenGL to both our source code and the linker before starting any actual work.\nFirst of all we need to make visible to our code the header files of SFML and OpenGL, in order to be able to use both APIs. On your Project Window on the left, right-click on the project’s name and click “Properties”. Go to “C++ Compiler” and copy the following “/System/Library/Frameworks/OpenGL.framework/Headers:/Library/Frameworks/SFML.framework/Headers” in the “Include Directories” textbox. Click on the Apply button.\nRight up next we need to guide the linker to the library files, otherwise messages of the type “cannot find the symbols defined” will populate the console. Go to “Linker”, click on “Libraries \u0026gt; Add Option \u0026gt; Add other option” and copy the value “-framework SFML -framework sfml-system -framework sfml-window -framework OpenGL”. This is a “tricky” part when setting up Netbeans on Mac OS because the –L option (which is used when adding libraries through the “Additional Library Directories”) simply does not work. Mac needs the -framework directive to guide it to the library files.\nSample Program Execution Everything should be fine by now. Copy and paste the contents of the main function found here http://www.sfml-dev.org/tutorials/1.6/sources/window-opengl.cpp into the main method of our project an execute the program. You should be able to see a white rotating cube. If that’s the case OpenGL and SFML are set up successfully in your Mac/NetBeans configuration!\nAdditional configuration Header files In case you structure your code in multiple directories (and you’ll probably need that once you start working on something larger than the introductory all-in-one tutorial) you’ll need to add the folders where you keep your header files in the “C++ Compiler \u0026gt; Include Directories”. This way you’ll avoid the usage of relative paths in the inclusion of header files in your project files and the additional effort to change paths if you move header files from one place to another. This could get cumbersome as the project gets bigger.\nResource files I also assume that you’ll also have resource files that need to be deployed along with the executable like configuration files, images, sounds etc. All you need is a little unix command line knowledge and a few edits in the makefile. Suppose that you have a keyboard.xml file under the folder (actual file system) \u0026lt;NETBEANS_PROJECT_FOLDER\u0026gt;/resources all you need to do is add the following line\n1  cp resources/keyboard.xml ${CND_DISTDIR}/${CND_CONF}/${CND_PLATFORM}/   in the makefile just below the .build-post: .build-impl goal\nNotes I’m completely new on NetBeans and relatively new on MAC so any comments are welcome. I’ve used NetBeans 7.2.1 C++ bundle and SFML 1.6 on a Mac OS X 10.7.5.\n","description":"OpenGL development in MacOS using NetBeans, C++ and SFML","id":6,"section":"posts","tags":["opengl","c++","mac"],"title":"MacOS X and OpenGL","uri":"https://zisistach.org/posts/02_mac_opengl/"},{"content":"Spring aop fun! My everyday life at work involves dealing with several Java frameworks such as Spring, Hibernate and JSF amongst others. Some of them are quite easy to grasp while others have a steeper learning curve (Hibernate raises its hand here), especially when it comes to making it performant.\nLately, I’ve been trying to extend the use of AOP in our project and use it for a quite common purpose, to measure the performance of our app. A simple google search will return a lot of sample implementations to give you an idea. In essence by using AOP and the @Around advice I managed to get the execution times of most of our service calls and with the help of some custom code to get the percentages that each part of our app was contributing to the overall time of execution. It’s true that the more you use Spring the more you appreciate the benefits derived from it.\nEverything was working just fine and we were all happy until a colleague of mine turned to me saying\n “You know, we’ve got to add this service’s methods to the calculations, I think that’s the one that’s slowing us down”\n and I thought to myself\n “Well, that’s easy, I’ll just add another one aop:pointcut with the corresponding expression and link that to a new aop:around advice”.\n In reality things are just so simple but my reality was quite different and cost me a whole working day to get it fixed.\nErroneous behavior As soon as the pointcut and advice were added errors of the following type started cropping up\n1 2 3 4 5  Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No matching bean of type [com.mycompany.StupidService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)}   or\n1 2 3 4 5 6 7  Caused by: org.springframework.beans.factory.BeanCreationException: Could not autowire field: private com.mycompany.StupidService; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No matching bean of type [com.mycompany.StupidService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)}   or\n1 2 3 4  Caused by: java.lang.IllegalStateException: Cannot convert value of type [org.springframework.aop.aspectj.AspectJExpressionPointcut] to required type [com.mycompany.StupidService] for property ‘stupidService’: no matching editors or conversion strategy found   Digging in I started an intensive search in the net with the above exceptions and although I found a lot of references and provided solutions none of them really fitted my case.\nThe bean I was injecting was definitely there (StupidServiceImpl.java) and it was definitely implementing the required interface (StupidService.java). Spring uses dynamic JDK proxies by default intercepting interface methods only. Moreover all references inside the code were referring to the interface and not the implementation. Not to mention other services will the same logic (as far as setup is concerned) were successfully injected. I tried everything JDK-proxies, CGILIB proxies, read again and again the Spring AOP documentation but to no avail. I even wrote a BeanFactoryPostProcessor to understand what was happening during Spring initialization and it was then that I realized that StupidService was initialized as an AspectJExpressionPointcut and not a StupidService instance and reconsidered the meaning behind the above exception.\nSolution What was the case? My StupidServiceImpl bean was annotated as\n1  @Service(“stupidService”)   while my aop:pointcut was declared inside applicationContext.xml as follows\n1  \u0026lt;aop:pointcut id=”stupidService” expression=”execution(* com.mycompany.stupidservice.StupidService.*(..))”/\u0026gt;   The id of the Service in the annotation was the same as the pointcut’s ID in the XML but neither Spring nor my IDE gave me an error that 2 Spring “elements” shared the same id. The end result was that StupidService was instantiated as a pointcut (annotations are parsed first, then xml configuration files) and when I was trying to inject it on my beans it could not be converted to a StupidService bean.\nIt seems that while Spring validates between bean ids in annotations and xml files (Spring flags an error if one bean in an XML has the same id with an annotated bean), it does not do so when it comes to annotations and aop advices declared in an XML file.\nTakeaways Can the above be considered as a Spring bug?\nAlways make sure that the IDs of your beans are unique. Usually your IDE will take good care of this job but as it seems this is not always the case. Pay extra attention to the errors displayed. If you see a failure in casting a bean from one type to another maybe you have fallen to the same “trap”.\nPS. For the record, I’m using Spring 3.0.5.RELEASE version.\n","description":"A case of name collision in Spring Framework","id":7,"section":"posts","tags":["spring","aop"],"title":"Spring AOP \u0026 Bean Naming. Is it a bug?","uri":"https://zisistach.org/posts/01_spring-aop-bug/"}]