[{"content":"Introduction We live in a software world where distributed applications are common. They usually include lots of independently deployed services that communicate with each other. Sure, there are many benefits from this approach, but it goes without saying that it also introduces some new problems that need our attention.\nOne of those is fault tolerance. For example, what happens when one of the services degrades in performance? If you don\u0026rsquo;t take any measures the application that makes the calls to that \u0026ldquo;problematic\u0026rdquo; service will start experiencing long timeouts and might even go down if threads accumulate while waiting for a response. You need to have some safeguards in-place.\nIt\u0026rsquo;s so common of a problem that there are open source tools available like Hystix (https://github.com/Netflix/Hystrix) and resilience4j (https://github.com/resilience4j/resilience4j) that deal with it. And most of the times taking advantage of such battle-tested tools is the way to go.\nHere, we\u0026rsquo;ll look at what might lurk inside such a fault-tolerant library. Think of it more like an exercise but it can also prove useful if you need something in-place without adding more dependencies in an existing library-bloated application. (Anyone having lived through JAR hell https://en.wikipedia.org/wiki/Java_Classloader#JAR_hell can relate to that)\nThe basic ideas behind this are described in https://github.com/Netflix/Hystrix/wiki#how-does-hystrix-accomplish-its-goals\nInitial take Imagine a scenario where you have a web application running on an application server like Apache Tomcat. Every click on the website is picked up by a thread from Tomcat\u0026rsquo;s thread pool. At some point, in order to accomplish a task it has to communicate with a Service through an API call like\n1  ServiceClient.serve(req);   The ServiceClient performs HTTP calls to the independent service. If the Service degrades then more and more Tomcat threads are getting piled up waiting for the Service to respond. In a busy web server this behavior can even bring down the server due to the high number of threads. Other symptoms include increased memory usage, increased CPU context switching or even inability to create new threads.\nSetting an upper bound You can put a limit to that by wrapping these requests to the Service in another thread. One way to go about this is by using a bounded thread pool in Java like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  final int COMPUTATION_TIMEOUT = 5; final int NUM_THREADS = 10; ExecutorService executor = Executors.newFixedThreadPool(NUM_THREADS); Supplier\u0026lt;Response\u0026gt; req = () -\u0026gt; { try { return ServiceClient.serve(requestParams); } catch (Exception e) { throw new CompletionException(e); } }; CompletableFuture\u0026lt;Response\u0026gt; result = CompletableFuture.supplyAsync(req, executor); return result.get(COMPUTATION_TIMEOUT, TimeUnit.SECONDS);   If that was written in plain English, it\u0026rsquo;d go like\n \"Look, I won't allow your Service to bring me down. You can have at most 10 simultaneous requests to the Service but no more. And each one of these will be allowed a max of 5 sec to complete. If it does not complete in 5sec you'll get a `TimeoutException` and the Tomcat thread will be allowed to return immediately.\"\n This is certainly a step in the right direction but we still have a blind spot here. What happens if there\u0026rsquo;s a surge of activity and these 10 threads are not enough to handle the extra traffic? Checking the JavaDoc of .newFixedThreadPool() it appears that Java will add those extra tasks into an unbounded queue. The word unbounded just smells trouble. We might end-up putting extreme pressure on the web server by letting this queue go wild. Now, what are our options?\nOne step further Depending on your use-case it might be proper to define a different strategy for queuing. One such is rejecting the incoming requests when all threads are busy might be a viable solution. For example, If you click to reserve a room or perform a search operation you can still get a message \u0026ldquo;Please retry\u0026rdquo; and issue the command again.\nThankfully you can configure this behavior by introducing a custom ThreadPoolExecutor\n1 2 3 4 5 6  ThreadPoolExecutor executor = new ThreadPoolExecutor( corePoolSize, maxNumThreads, keepAliveTime, keepAliveTimeUnit, queue);   The above example creates a DirectHandOff type pool. Having\ncorePoolSize=20, maxNumThreads=100, keepAliveTime=1, keepAliveTimeUnit=TimeUnit.MINUTES, queue=new SynchronousQueue\u0026lt;\u0026gt;() the above translates into English as follows\n \"The pool starts with 20 threads. If all 20 threads are busy and new requests keep coming in, new threads will be spawned till the upper limit of 100 threads is reached. If all 100 threads are busy new requests will be rejected with a `RejectedExecutionException`. If the load decreases the pool will start killing the threads that sit idle for more than 1min until it goes back to 20 threads.\"\n A nice \u0026ldquo;side-effect\u0026rdquo; of the above ThreadPoolExecutor is that the size of the pool is adaptable to the web server\u0026rsquo;s traffic by fluctuating between corePoolSize and maxNumThreads depending on the load.\nOne last thing. In practice it\u0026rsquo;s also useful to give names to those threads. You can thank yourself later while debugging and reading thread dumps. A custom ThreadPoolExecutor is all that is needed\n1 2 3 4 5 6 7 8 9 10 11  // ThreadFactoryBuilder is used here for simplicity, it\u0026#39;s a Google Guava class ThreadFactory threadFactory = new ThreadFactoryBuilder() .setNameFormat(poolName + \u0026#34;-%d\u0026#34;) .build(); ThreadPoolExecutor executor = new ThreadPoolExecutor( corePoolSize, maxNumThreads, keepAliveTime, keepAliveTimeUnit, new SynchronousQueue\u0026lt;\u0026gt;(), threadFactory);   Linking your work to a circuit breaker Finally, after all the above are setup properly you can combine your new thread pool with a circuit breaker. A simplified circuit breaker records successes and failures and has 2 corresponding thresholds. When there are FAILURE_THRESHOLD number of consecutive failed requests the circuit is open, meaning new requests are not forwarded to the service. The circuit remains semi-open for DELAY number of TIME_UNIT. Once you get SUCCESS_THRESHOLD number of consecutive successful request the circuit will move from semi-open to close and communication with the service resumes normally.\nThe only thing that needs care is when to increase the counter of failed requests. Usually certain exceptions should be taken into account, in our case these include RejectedExecutionException and TimeoutException.\n1 2 3 4 5  CircuitBreaker\u0026lt;Object\u0026gt; breaker = new CircuitBreaker\u0026lt;\u0026gt;() .withFailureThreshold(FAILURE_THRESHOLD, 20)) .withSuccessThreshold(SUCCESS_THRESHOLD, 10)) .withDelay(Duration.ofMillis(DELAY, 20, TIME_UNIT))); } }   Downsides The above approach will work just fine. The main downsides is that the limits defined in ThreadPoolExecutor and CircuitBreaker are predefined and static. You need to understand your system\u0026rsquo;s capacity and your expected traffic for setting the proper values without hindering your system\u0026rsquo;s scalability.\nIn order to overcome this you need to work towards more adaptive implementations that react to an application\u0026rsquo;s real-time performance.\nTakeaways Communication between services is of critical importance. Especially nowadays with applications having many moving parts, a single failure can cascade to the whole system. We must be very prudent when designing such a system.\nOne cannot simply deploy a bunch of services that communicate with each other in order to bring something meaningful to the user and just hope for the best. It\u0026rsquo;s better to assume that something will fail and be sure that most (if not all) of the times it will.\nThe basic steps that have discussed to achieve basic fault tolerance in your application are\n Wrap each request in a new thread Make sure your thread is part of a bounded thread pool Set timeouts for each request Reject additional requests when the threshold is exceeded Trigger a circuit breaker in case of failures  ","description":"Safeguard your application from total breakdown","id":1,"section":"posts","tags":["java","concurrency","distributed"],"title":"Fault tolerant services","uri":"https://zisistach.org/posts/05_fault_tolerance/"},{"content":"Introduction We are so spoiled by Java\u0026rsquo;s concurrency utilities that it\u0026rsquo;s easy to ignore some basic error handling like what happens when some of the threads we have created \u0026ldquo;go wild\u0026rdquo;.\nIt\u0026rsquo;s like getting on a fast motorcycle for the first time. Thrilled by the idea of speed you roll on the throttle reaching \u0026gt;200Km/h in no time but then a car crops up out of nowhere. It\u0026rsquo;s only then that you realize you have no idea how to apply the brakes. Nasty situation with a bad ending most of the times.\nSimilarly with concurrency, you can get something \u0026ldquo;working\u0026rdquo; pretty quick, but you still have to put some thought on how to deal with those threads in case something goes wrong!\nVanilla scenario The most common examples involving threads are trivial in the sense that\n A thread pool is created Some relatively simple tasks are submitted to it Shutdown is issued in the end All results are available and you move on  That could look like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  import org.apache.commons.lang3.RandomStringUtils; import java.util.ArrayList; import java.util.List; import java.util.concurrent.*; public class InterruptionPolicyExample { public static void main(String[] args) { ExecutorService service = Executors.newSingleThreadExecutor(); List\u0026lt;Future\u0026lt;String\u0026gt;\u0026gt; futures = new ArrayList\u0026lt;\u0026gt;(); futures.add(service.submit(() -\u0026gt; { StringBuilder testString = new StringBuilder(); testString.append(RandomStringUtils.randomAlphabetic(100)); return testString; })); gatherResults(futures); shutdownAndAwaitTermination(service); } private static void gatherResults(List\u0026lt;Future\u0026lt;String\u0026gt;\u0026gt; futures) { for (Future\u0026lt;String\u0026gt; future : futures) { try { String result = future.get(); System.out.println(\u0026#34;result = \u0026#34; + result); } catch (InterruptedException | ExecutionException e) { System.out.println(e.getClass()); } } } static void shutdownAndAwaitTermination(ExecutorService pool) { pool.shutdown(); // Disable new tasks from being submitted  try { // Wait a while for existing tasks to terminate  if (!pool.awaitTermination(10, TimeUnit.SECONDS)) { pool.shutdownNow(); // Cancel currently executing tasks  // Wait a while for tasks to respond to being cancelled  if (!pool.awaitTermination(10, TimeUnit.SECONDS)) System.err.println(\u0026#34;Pool did not terminate\u0026#34;); } } catch (InterruptedException ie) { // (Re-)Cancel if current thread also interrupted  pool.shutdownNow(); // Preserve interrupt status  Thread.currentThread().interrupt(); } } }   We create a single-threaded pool and submit a task that just creates and returns a random String. Everything goes smooth. One worthy point to note here is that we need to properly clean-up after our thread pool by calling .shutdownAndAwaitTermination() , otherwise the program will never exit.\nGetting busy Imagine now that the logic inside your thread enters an infinite while loop or gets into a situation that involves excessive CPU or memory usage. We\u0026rsquo;ll keep it simple here and go with a nice traditional infinite loop\n1 2 3 4 5 6  futures.add(service.submit(() -\u0026gt; { StringBuilder test = new StringBuilder(); while(1) { test.append(RandomStringUtils.randomAlphabetic(100)); } }));   The above can bring a server down on its own. Imagine an operation in the real world that operates on character level and accepts tons of text in a single request. Now on top of that picture a busy web application which accepts lots of similar requests and you have a recipe for disaster. Threads are busy consuming resources and your web server eventually crashes. Now what?\nPlease respond cut #1 First reaction is to add a timeout when waiting for the thread\u0026rsquo;s result as in\n1  String result = future.get(5000, TimeUnit.MILLISECONDS);   Still the program hangs. We got the TimeoutException alright but the thread keeps running and chipping away memory.\nPlease respond cut #2 OK, Timeout won\u0026rsquo;t do it. Let\u0026rsquo;s be more pushy on that and also cancel the task - which sends an interruption message - after the timeout of 5sec.\n1 2 3 4 5 6 7  try { String result = future.get(5000, TimeUnit.MILLISECONDS); } catch (InterruptedException | ExecutionException | TimeoutException e) { System.out.println(e.getMessage); } finally { future.cancel(true); }   Still nothing, but why? Java can and will deliver the interruption to the corresponding thread but you still must be responsive to interruption. We know that blocking methods are responsive to interruption (like .wait() or BlockingQueue\u0026rsquo;s put() and take() ) but what about threads that do not make use of such methods.\nPlease respond cut #3 Remember that interruption is a co-operative mechanism, so we have to do our part. In order to make the above example work we need to react on the interruption\n1 2 3 4 5 6 7 8 9 10 11 12 13  futures.add(service.submit(() -\u0026gt; { StringBuilder test = new StringBuilder(); while(1) { test.append(RandomStringUtils.randomAlphabetic(100)); if (Thread.currentThread().isInterrupted()) { Thread.currentThread().interrupt(); break; } }\treturn \u0026#34;Finished work in thread\u0026#34;; }));   The interrupted status of the thread is set by future.cancel(true) but one has to check for this using Thread.currentThread().isInterrupted() and act accordingly. You also have to set the interrupted status back to the original value since we are not the owners of the thread.\nTakeaways Your concurrent applications will be far more robust if you ensure the following\n Make sure your threads are responsive to interruption Never forget to shutdown your thread pools, or register them to get called during JVM shutdown  Your need to tick both boxes above in order to properly clean up your threads and exit the JVM. This is especially important in CLI tools for example which run for some time and exit after each call. For long running applications like a web server you can register your shutdown hook through Runtime.getRuntime().addShutdownHook.\n","description":"Be gentle to interruptions","id":2,"section":"posts","tags":["java","concurrency"],"title":"Keeping your threads under control","uri":"https://zisistach.org/posts/04_be_gentle_to_interruptions/"},{"content":"Written in Go, Hugo is an open source static site generator available under the Apache Licence 2.0. Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.\nHugo makes use of a variety of open source projects including:\n https://github.com/russross/blackfriday https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper  Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.\nHugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.\nWebsites built with Hugo are extremelly fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.\nLearn more and contribute on GitHub.\n","description":"Hugo, the world’s fastest framework for building websites","id":3,"section":"","tags":null,"title":"About","uri":"https://zisistach.org/about/"},{"content":"Introduction In this post we’ll focus on SolrCloud stability and search performance. Most of the lessons have been learned the hard way, fighting them on production some times. The list is nowhere complete and rest assured that there are more Solr performance related storied to tell. Take notte that the ones discussed below are not covered in great detail as it\u0026rsquo;d make a very long post. Some of them are even worth their own individual post.\nPotential issues causing cluster instability The following points fall under the “abusing Solr” category. The same way that JOIN functionality provided by RDBMs does not mean that all the JOIN queries you can imagine will be performant, there are some usages of Solr that are not ideal performance-wise.\nAvoid search calls using .setRows(Integer.MAX) Avoid setting Integer.MAX or high values (usually when doing paging with start and rows params) in .setRows() call when preparing a SolrQuery. It can wreak havoc to the Solr cluster. Basically Solr tries to allocate as much memory as indicated in the parameter passed to .setRows(), not a good thing. Haven’t seen it with my own eyes happening but others have. Check Solr Wiki and Statsbiblioteket.\nAlternatives:\n For real-time requests first get the # of docs in the index and pass this to .setRows() instead. For calls that you know what to expect just pass the expected number of results. For “next-page” functionality and offline procedures cursorMark can be utilized.  Avoid using deleteByQuery Mixing indexing calls with deleteByQuery() commands is just asking for trouble. If you witness seemingly random replicas crashes and/or getting in recovery mode it might be related to deleteByQuery interspersed with indexing requests.\nAlternatives:\n Make use of deleteById() directly if possible. Try to search first using the query that is passed to deleteByQuery(), get the IDs and call deleteById(). Get rid of your deletes altogether, by making use of atomic updates. This comes with its own limitations but you can check if it fits your use-case.  Improving search throughput Adjust maxConnectionsPerHost to your needs The story has as follows. Search on a single-sharded SolrCloud setup with an 10M docs/350GB index and high cardinality fields was slow, especially when faceting was utilized. A simple search from Solr admin page with faceting enabled could take ~1.5sec.After the index was split into 8 shards the same query’s response time dropped to ~200msec! That’s great I thought, problem solved. But it wasn’t to be. Running a load test showed that after a certain rate (\u0026gt;20QPS) performance dropped dramatically.So, what was going on? At first I noticed that having more than 2 shards on the same Solr JVM was causing slowness, but I could not figure out why this was happening. The CPUs on the machine were more than enough to server requests in multiple shards. Turns out there’s a property which can be placed inside solr.xml under the HttpShardHandlerFactory node. It’s called maxConnectionsPerHost and increasing it (default value is 20) gives some fresh air to Solr and inter-shard communication and response times are back to expected values (~200msec) even under heavy load.\nCheck your FilterCache hit ratio (especially in distributed search) Searching in a distributed index makes heavy use of the FilterCache that is not evident when searching in a single shard. Faceting in specific seems to generate and store multiple entries in the FilterCache. In these cases it’s better to configure your FilterCache using maxRamMB as faceting adds multiple entries of small size.\nProfile indexing/search and pay attention to your custom analyzers/tokenizers/filters You don’t really need any exotic tools for this one. Tools provided by the JDK (like JVisualVM) can monitor threads/memory activity while you index/search. Look for your custom classes and how much time is spent on them.\nIf you have GC choking issues, try the G1 collector. It can do miracles. Witnessed a setup where the default collector used by SolrCloud could not cope with the load after some time. Just switching to G1 fixed things. GC tuning is very dependent on each use case so below only the most important parameters are mentioned. These include\n -XX:+ParallelRefProcEnabled, enable it when Ref Proc and Ref Enq phases take too much time. More details at https://docs.oracle.com/javase/9/gctuning/garbage-first-garbage-collector-tuning.htm#JSGCT-GUID-90E30ACA-8040-432E-B3A0-1E0440AB556A -XX:MaxGCPauseMillis the maximum GC pause duration that G1 should try to achieve. Don\u0026rsquo;t be too optimistic on this one and set a very low value as G1 might struggle in trying to keep up. -XX:ParallelGCThreads which sets the number of threads used during parallel phases of the garbage collectors. If you have the CPU try to increase it, it can have a big impact on performance. -XX:G1HeapRegionSize which is depended on the total heap space allocated. The target is to have ~2048 G1 regions according to the official documentation at http://www.oracle.com/technetwork/articles/java/g1gc-1984535.html  Utilize facet.threads In practice the use of facet.threads has improved search response times when faceting is enabled but still sharding the index will probably yield better response times for faceting and search in general. There’s no point in setting this variable a higher value than the # of fields you are faceting on. It should at most be equal to the # of faceting fields.\nUse DocValues for faceting This comes directly from Solr Documentation, although I don’t have any serious metrics I’ll take their word for it.\nPay attention to your deleted documents Maybe you have a perfectly healthy SolrCloud instance which performs great. Time goes by and after a few months you notice a dramatic degradation in search performance. Code is the same, hardware is the same, traffic is the same, query and indexing patterns are the same. It’s time to have a look at the percentage of the deleted documents in your index. It might be responsible for this degradation and you have to somehow limit it.\nLucene - and therefore Solr - make use of certain append-only data structures when storing terms that in the index segments. More details on the underlying data structures (SSTables), how they work and what are the benefits can be found in Chapter 3 of the great Designing Data-Intensive Applications book (https://dataintensive.net/)\nThis means that whenever something is deleted it is just marked as such and it\u0026rsquo;s not actually removed from the index. Unsurprisingly your index size grows, as more and more documents are getting deleted. And if that wasn\u0026rsquo;t enough, your search queries are also getting slower as the reader has to traverse more documents. In the meantime, the so-called merger that runs periodically in the background is responsible for merging segments (and removing the deleted entries) once the deletion ratio goes above a certain threshold. But for some use-cases this value might prove to be too high and it\u0026rsquo;s not configurable.\nSegments handling in Lucene/Solr as well as handling of deleted documents is a black art (or black magic for that matter). More info on how merging can be enforced at https://sematext.com/blog/solr-optimize-is-not-bad-for-you-lucene-solr-revolution/. There is an HTTP API that can get you out of this mess. In later versions support is coming for configuring this threshold.\nNote: All of the above were tested on SolrCloud 7.4.1, so some of those hints may not be applicable in recent versions.\n","description":"A guide on common performance issues encountered in SolrCloud","id":5,"section":"posts","tags":["solrcloud","lucene","java","performance","distributed"],"title":"SolrCloud performance tuning","uri":"https://zisistach.org/posts/03_solr_performance/"},{"content":"Introduction Here is a guide on configuring an SFML, OpenGL, C++ project under Netbeans in MacOS X. I couldn’t find any similar guide on the net and I encountered some caveats and intricacies while trying to set it up. The lack of a complete guide alone made it worth blogging about.\nFirst of all, I assume that NetBeans and SFML are already installed on your machine. If not, please go to the corresponding product’s official site and follow the guides there. They are pretty straightforward. Also make sure that Xcode has the command-line tools installed in order to ensure that the g++ compiler is properly installed and configured. Once everything is in place let’s start setting up the project.\nCreating the NetBeans project Open Netbeans and click on “File \u0026gt; New Project…”. From the available options select the “C/C++ Application” and click “Next”.\nIn the following screen provide a name for your project in the “Project Name:” textbox and click on “Finish”. The default values for remaining options should be just fine.\nConfiguring the needed libraries We need to make visible SFML and OpenGL to both our source code and the linker before starting any actual work.\nFirst of all we need to make visible to our code the header files of SFML and OpenGL, in order to be able to use both APIs. On your Project Window on the left, right-click on the project’s name and click “Properties”. Go to “C++ Compiler” and copy the following “/System/Library/Frameworks/OpenGL.framework/Headers:/Library/Frameworks/SFML.framework/Headers” in the “Include Directories” textbox. Click on the Apply button.\nRight up next we need to guide the linker to the library files, otherwise messages of the type “cannot find the symbols defined” will populate the console. Go to “Linker”, click on “Libraries \u0026gt; Add Option \u0026gt; Add other option” and copy the value “-framework SFML -framework sfml-system -framework sfml-window -framework OpenGL”. This is a “tricky” part when setting up Netbeans on Mac OS because the –L option (which is used when adding libraries through the “Additional Library Directories”) simply does not work. Mac needs the -framework directive to guide it to the library files.\nSample Program Execution Everything should be fine by now. Copy and paste the contents of the main function found here http://www.sfml-dev.org/tutorials/1.6/sources/window-opengl.cpp into the main method of our project an execute the program. You should be able to see a white rotating cube. If that’s the case OpenGL and SFML are set up successfully in your Mac/NetBeans configuration!\nAdditional configuration Header files In case you structure your code in multiple directories (and you’ll probably need that once you start working on something larger than the introductory all-in-one tutorial) you’ll need to add the folders where you keep your header files in the “C++ Compiler \u0026gt; Include Directories”. This way you’ll avoid the usage of relative paths in the inclusion of header files in your project files and the additional effort to change paths if you move header files from one place to another. This could get cumbersome as the project gets bigger.\nResource files I also assume that you’ll also have resource files that need to be deployed along with the executable like configuration files, images, sounds etc. All you need is a little unix command line knowledge and a few edits in the makefile. Suppose that you have a keyboard.xml file under the folder (actual file system) \u0026lt;NETBEANS_PROJECT_FOLDER\u0026gt;/resources all you need to do is add the following line\n1  cp resources/keyboard.xml ${CND_DISTDIR}/${CND_CONF}/${CND_PLATFORM}/   in the makefile just below the .build-post: .build-impl goal\nNotes I’m completely new on NetBeans and relatively new on MAC so any comments are welcome. I’ve used NetBeans 7.2.1 C++ bundle and SFML 1.6 on Mac OS X 10.7.5.\n","description":"OpenGL development in MacOS using NetBeans, C++ and SFML","id":8,"section":"posts","tags":["opengl","c++","mac"],"title":"MacOS X and OpenGL","uri":"https://zisistach.org/posts/02_mac_opengl/"},{"content":"Spring AOP fun! My everyday life at work involves dealing with several Java frameworks such as Spring, Hibernate and JSF among others. Most frameworks Take Spring for example. IT removes a lot of burden from the developer and gets you up to speed fast but in order to achieve this it makes a lot of conventions. This is not bad per-se but it can turn into an ugly beast at times - notably for the uninitiated - when trying to decipher what is wrong in your application.\nLately, I’ve been trying to extend the use of Spring AOP in our project and utilize it to measure the performance of our app. A simple google search will return a lot of sample implementations to give you an idea. In essence by using AOP and the @Around advice I managed to get the execution times of most of our service calls. A little bit of extra custom code and we also got the percentages that each part of our app was contributing to the overall time of execution. It’s true that the more you are using Spring the more you appreciate its benefits.\nEverything was working just fine and we were all happy until a colleague of mine turned to me saying\n “You know, we’ve got to add this service’s methods to the calculations, I think that’s the one that’s slowing us down”\n and I thought to myself\n “Well, that’s easy, I’ll just add another one aop:pointcut with the corresponding expression and link that to a new aop:around advice”.\n In reality it should have been as simple as that but I\u0026rsquo;ve ended up spending a whole working day to sort it out.\nErroneous behavior As soon as the pointcut and advice were added errors of the following type started cropping up\n1 2 3 4 5  Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No matching bean of type [com.mycompany.StupidService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)}   or\n1 2 3 4 5 6 7  Caused by: org.springframework.beans.factory.BeanCreationException: Could not autowire field: private com.mycompany.StupidService; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No matching bean of type [com.mycompany.StupidService] found for dependency: expected at least 1 bean which qualifies as autowire candidate for this dependency. Dependency annotations: {@org.springframework.beans.factory.annotation.Autowired(required=true)}   or\n1 2 3 4  Caused by: java.lang.IllegalStateException: Cannot convert value of type [org.springframework.aop.aspectj.AspectJExpressionPointcut] to required type [com.mycompany.StupidService] for property ‘stupidService’: no matching editors or conversion strategy found   Digging in I started an intensive internet search with the above exceptions and although I found a lot of references and provided solutions none of them really fitted my case.\nThe bean I was injecting was definitely there (StupidServiceImpl.java) and it was definitely implementing the required interface (StupidService.java). Spring uses dynamic JDK proxies by default intercepting interface methods only. Moreover all references inside the code were referring to the interface and not the implementation. Not to mention other services will the same logic (as far as setup is concerned) were successfully injected. I tried everything JDK-proxies, CGILIB proxies, read again and again the Spring AOP documentation but to no avail. I even wrote a BeanFactoryPostProcessor to understand what was happening during Spring initialization and it was then that I realized that StupidService was initialized as an AspectJExpressionPointcut and not as a StupidService instance and reconsidered the meaning behind the above exception.\nSolution What was the case? My StupidServiceImpl bean was annotated as\n1  @Service(“stupidService”)   while my aop:pointcut was declared inside applicationContext.xml as follows\n1  \u0026lt;aop:pointcut id=”stupidService” expression=”execution(* com.mycompany.stupidservice.StupidService.*(..))”/\u0026gt;   The id of the Service in the annotation was the same as the pointcut’s ID in the XML but neither Spring nor my IDE gave me an error that 2 Spring “elements” shared the same id. The end result was that StupidService was instantiated as a pointcut (annotations are parsed first, then xml configuration files) and when I was trying to inject it on my beans it could not be converted to a StupidService bean.\nTakeaways Why the hell should someone give the same name to an AOP advice and a bean?\nShould Spring provide a more informative message?\n It seems that while Spring validates between bean ids in annotations and xml files (Spring flags an error if one bean in an XML has the same id with an annotated bean), it does not do so when it comes to annotations and aop advices declared in an XML file. Always make sure that the IDs of your beans are unique.  Usually your IDE will take good care of this job but as it seems this is not always the case. Pay extra attention to the errors displayed. If you see a failure in casting a bean from one type to another maybe you have fallen to the same “trap”.\nPS. The above were observed in Spring 3.0.5.RELEASE.\n","description":"A case of name collision in Spring Framework","id":9,"section":"posts","tags":["spring","java","aop"],"title":"Spring AOP \u0026 Bean Naming conflict","uri":"https://zisistach.org/posts/01_spring-aop-bug/"}]